@article{larsen2024synchronized,
  author    = {Larsen, O. F. P. and Tresselt, W. G. and Lorenz, E. A. and Holt, T. and Sandstrak, G. and Hansen, T. I. and Su, X. and Holt, A.},
  title     = {{A Method for Synchronized Use of EEG and Eye Tracking in Fully Immersive VR}},
  journal   = {Frontiers in Human Neuroscience},
  volume    = {18},
  pages     = {1347974},
  year      = {2024},
  doi       = {10.3389/fnhum.2024.1347974}
}

@article{khazi2012eeg,
  author    = {Khazi, M. and Kumar, A. and J, V. M.},
  title     = {{Analysis of EEG Using 10:20 Electrode System}},
  journal   = {International Journal of Innovative Research in Science, Engineering and Technology},
  volume    = {1},
  number    = {2},
  year      = {2012}
}


@Article{s19061423,
AUTHOR = {Padfield, Natasha and Zabalza, Jaime and Zhao, Huimin and Masero, Valentin and Ren, Jinchang},
TITLE = {EEG-Based Brain-Computer Interfaces Using Motor-Imagery: Techniques and Challenges},
JOURNAL = {Sensors},
VOLUME = {19},
YEAR = {2019},
NUMBER = {6},
ARTICLE-NUMBER = {1423},
URL = {https://www.mdpi.com/1424-8220/19/6/1423},
PubMedID = {30909489},
ISSN = {1424-8220},
ABSTRACT = {Electroencephalography (EEG)-based brain-computer interfaces (BCIs), particularly those using motor-imagery (MI) data, have the potential to become groundbreaking technologies in both clinical and entertainment settings. MI data is generated when a subject imagines the movement of a limb. This paper reviews state-of-the-art signal processing techniques for MI EEG-based BCIs, with a particular focus on the feature extraction, feature selection and classification techniques used. It also summarizes the main applications of EEG-based BCIs, particularly those based on MI data, and finally presents a detailed discussion of the most prevalent challenges impeding the development and commercialization of EEG-based BCIs.},
DOI = {10.3390/s19061423}
}


@Article{s120201211,
AUTHOR = {Nicolas-Alonso, Luis Fernando and Gomez-Gil, Jaime},
TITLE = {Brain Computer Interfaces, a Review},
JOURNAL = {Sensors},
VOLUME = {12},
YEAR = {2012},
NUMBER = {2},
PAGES = {1211--1279},
URL = {https://www.mdpi.com/1424-8220/12/2/1211},
PubMedID = {22438708},
ISSN = {1424-8220},
ABSTRACT = {A brain-computer interface (BCI) is a hardware and software communications system that permits cerebral activity alone to control computers or external devices. The immediate goal of BCI research is to provide communications capabilities to severely disabled people who are totally paralyzed or ‘locked in’ by neurological neuromuscular disorders, such as amyotrophic lateral sclerosis, brain stem stroke, or spinal cord injury. Here, we review the state-of-the-art of BCIs, looking at the different steps that form a standard BCI: signal acquisition, preprocessing or signal enhancement, feature extraction, classification and the control interface. We discuss their advantages, drawbacks, and latest advances, and we survey the numerous technologies reported in the scientific literature to design each step of a BCI. First, the review examines the neuroimaging modalities used in the signal acquisition step, each of which monitors a different functional brain activity such as electrical, magnetic or metabolic activity. Second, the review discusses different electrophysiological control signals that determine user intentions, which can be detected in brain activity. Third, the review includes some techniques used in the signal enhancement step to deal with the artifacts in the control signals and improve the performance. Fourth, the review studies some mathematic algorithms used in the feature extraction and classification steps which translate the information in the control signals into commands that operate a computer or other device. Finally, the review provides an overview of various BCI applications that control a range of devices.},
DOI = {10.3390/s120201211}
}

@article{Vasiljevic20012020,
author = {Gabriel Alves Mendes Vasiljevic and Leonardo Cunha de Miranda},
title = {Brain–Computer Interface Games Based on Consumer-Grade EEG Devices: A Systematic Literature Review},
journal = {International Journal of Human–Computer Interaction},
volume = {36},
number = {2},
pages = {105--142},
year = {2020},
publisher = {Taylor \& Francis},
doi = {10.1080/10447318.2019.1612213},
URL = {https://doi.org/10.1080/10447318.2019.1612213},
eprint = {https://doi.org/10.1080/10447318.2019.1612213}
}

@inproceedings{Pan2017EvaluationOC,
  title={Evaluation of Consumer-Grade EEG Headsets for BCI Drone Control},
  author={Peining Pan and Guojun Tan and Aung Aung Phyo Wai},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:55762335}
}

@INPROCEEDINGS{a10795393,
  author={Rizzo, Luigi and Zicari, Paolo and Cicirelli, Franco and Guerrieri, Antonio and Micieli, Massimo and Vinci, Andrea},
  booktitle={2024 IEEE Conference on Pervasive and Intelligent Computing (PICom)}, 
  title={A Study on Consumer-Grade EEG Headsets in BCI Applications}, 
  year={2024},
  volume={},
  number={},
  pages={67-74},
  keywords={Headphones;Electrodes;Electric potential;Feature extraction;Electroencephalography;Brain-computer interfaces;Classification algorithms;Brain-Computer Interface;Consumer-Grade EEG Headsets;Wearables;EEG Electrode Placement;Machine Learning;Comparative Study},
  doi={10.1109/PICom64201.2024.00016}}


@article{101371,
    doi = {10.1371/journal.pone.0104854},
    author = {Höhne, Johannes AND Holz, Elisa AND Staiger-Sälzer, Pit AND Müller, Klaus-Robert AND Kübler, Andrea AND Tangermann, Michael},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Motor Imagery for Severely Motor-Impaired Patients: Evidence for Brain-Computer Interfacing as Superior Control Solution},
    year = {2014},
    month = {08},
    volume = {9},
    url = {https://doi.org/10.1371/journal.pone.0104854},
    pages = {1-11},
    abstract = {Brain-Computer Interfaces (BCIs) strive to decode brain signals into control commands for severely handicapped people with no means of muscular control. These potential users of noninvasive BCIs display a large range of physical and mental conditions. Prior studies have shown the general applicability of BCI with patients, with the conflict of either using many training sessions or studying only moderately restricted patients. We present a BCI system designed to establish external control for severely motor-impaired patients within a very short time. Within only six experimental sessions, three out of four patients were able to gain significant control over the BCI, which was based on motor imagery or attempted execution. For the most affected patient, we found evidence that the BCI could outperform the best assistive technology (AT) of the patient in terms of control accuracy, reaction time and information transfer rate. We credit this success to the applied user-centered design approach and to a highly flexible technical setup. State-of-the art machine learning methods allowed the exploitation and combination of multiple relevant features contained in the EEG, which rapidly enabled the patients to gain substantial BCI control. Thus, we could show the feasibility of a flexible and tailorable BCI application in severely disabled users. This can be considered a significant success for two reasons: Firstly, the results were obtained within a short period of time, matching the tight clinical requirements. Secondly, the participating patients showed, compared to most other studies, very severe communication deficits. They were dependent on everyday use of AT and two patients were in a locked-in state. For the most affected patient a reliable communication was rarely possible with existing AT.},
    number = {8},

}

@ARTICLE{6451193,
  author={Kos'myna, Nataliya and Tarpin-Bernard, Franck},
  journal={IEEE Transactions on Computational Intelligence and AI in Games}, 
  title={Evaluation and Comparison of a Multimodal Combination of BCI Paradigms and Eye Tracking With Affordable Consumer-Grade Hardware in a Gaming Context}, 
  year={2013},
  volume={5},
  number={2},
  pages={150-154},
  keywords={Games;Context modeling;Eye tracking;Electrodes;Hardware;Tracking;Brain–computer interfaces (BCIs);eye tracking;multimodal interaction},
  doi={10.1109/TCIAIG.2012.2230003}}

@article{Sellers01102010,
author = {Eric W. Sellers and Theresa M. Vaughan and Jonathan R. Wolpaw},
title = {A brain-computer interface for long-term independent home use},
journal = {Amyotrophic Lateral Sclerosis},
volume = {11},
number = {5},
pages = {449--455},
year = {2010},
publisher = {Taylor \& Francis},
doi = {10.3109/17482961003777470},
note ={PMID: 20583947},
URL = {https://doi.org/10.3109/17482961003777470},
eprint = {https://doi.org/10.3109/17482961003777470}
}

@book{1011452,
author = {Jerald, Jason},
title = {The VR Book: Human-Centered Design for Virtual Reality},
year = {2015},
isbn = {9781970001129},
publisher = {Association for Computing Machinery and Morgan \& Claypool},
abstract = {Virtual reality (VR) can provide our minds with direct access to digital media in a way that seemingly has no limits. However, creating compelling VR experiences is an incredibly complex challenge. When VR is done well, the results are brilliant and pleasurable experiences that go beyond what we can do in the real world. When VR is done badly, not only is the system frustrating to use, but it can result in sickness. There are many causes of bad VR; some failures come from the limitations of technology, but many come from a lack of understanding perception, interaction, design principles, and real users. This book discusses these issues by emphasizing the human element of VR. The fact is, if we do not get the human element correct, then no amount of technology will make VR anything more than an interesting tool confined to research laboratories. Even when VR principles are fully understood, the first implementation is rarely novel and almost never ideal due to the complex nature of VR and the countless possibilities that can be created. The VR principles discussed in this book will enable readers to intelligently experiment with the rules and iteratively design towards innovative experiences.}
}

@book{laviola2017book,
  author    = {LaViola Jr., J. J. and Kruijff, E. and Bowman, D. A. and McMahan, R. P. and Poupyrev, I.},
  title     = {{3D User Interfaces: Theory and Practice}},
  publisher = {Addison-Wesley Professional},
  year      = {2017}
}

@article{101063,
    author = {Chong, Hwei Teeng and Lim, Chen Kim and Tan, Kian Lam},
    title = {Challenges in virtual reality system: A review},
    journal = {AIP Conference Proceedings},
    volume = {2016},
    number = {1},
    pages = {020037},
    year = {2018},
    month = {09},
    abstract = {Application of immersive Virtual Reality (VR) system has been proven to be practical and effective in offering the sense of presence and engagement for users to explore inside a virtual environment (VE) creation. However, numerous user experience problems still occur within the application design and development phase. An attempt to categorized the issues from presented studies by the researchers within the scope of papers between years of 2007 to 2017. There is no empirical evaluation conducted in this reviewing process but the aim is to bridge the gap between user experience and application system through understanding and learning from the challenges in the system itself. This can promote a more highly acceptance level of this complex technology among the users. This paper also proposes the future research directions and predicts the development trends of VR system.},
    issn = {0094-243X},
    doi = {10.1063/1.5055439},
    url = {https://doi.org/10.1063/1.5055439},
    eprint = {https://pubs.aip.org/aip/acp/article-pdf/doi/10.1063/1.5055439/14167135/020037\_1\_online.pdf},
}

@INPROCEEDINGS{9419261,
  author={Meier, Manuel and Streli, Paul and Fender, Andreas and Holz, Christian},
  booktitle={2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Demonstrating the Use of Rapid Touch Interaction in Virtual Reality for Prolonged Interaction in Productivity Scenarios}, 
  year={2021},
  volume={},
  number={},
  pages={761-762},
  keywords={Productivity;Headphones;Three-dimensional displays;Conferences;Prototypes;Virtual reality;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction techniques;Gestural input},
  doi={10.1109/VRW52623.2021.00263}}

@ARTICLE{9873986,
  author={Sidenmark, Ludwig and Parent, Mark and Wu, Chi-Hao and Chan, Joannes and Glueck, Michael and Wigdor, Daniel and Grossman, Tovi and Giordano, Marcello},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Weighted Pointer: Error-aware Gaze-based Interaction through Fallback Modalities}, 
  year={2022},
  volume={28},
  number={11},
  pages={3585-3595},
  keywords={Gaze tracking;Target tracking;Virtual reality;Switches;Manuals;Calibration;Measurement uncertainty;Eye tracking;Gaze interaction;Virtual Reality;Adaptive interfaces;Accessibility},
  doi={10.1109/TVCG.2022.3203096}}

@article{101167,
    author = {Martinez-Conde, Susana and Macknik, Stephen L.},
    title = {Fixational eye movements across vertebrates: Comparative dynamics, physiology, and perception},
    journal = {Journal of Vision},
    volume = {8},
    number = {14},
    pages = {28-28},
    year = {2008},
    month = {12},
    abstract = { During visual fixation, human eyes are never still. Instead, they constantly produce involuntary “fixational eye movements.” Fixational eye movements overcome neural adaptation and prevent visual fading: thus they are an important tool to understand how the brain makes the environment visible. The last decade has seen a growing interest in the analysis of fixational eye movements in humans and primates, as well as in their perceptual and physiological consequences. However, no comprehensive comparison of fixational eye movements across species has been offered. Here we review five decades of fixational eye movement studies in non-human vertebrates, and we discuss the existing evidence concerning their physiological and perceptual effects. We also provide a table that summarizes the physical parameters of the different types of fixational eye movements described in non-human vertebrates.},
    issn = {1534-7362},
    doi = {10.1167/8.14.28},
    url = {https://doi.org/10.1167/8.14.28},
    eprint = {https://arvojournals.org/arvo/content\_public/journal/jov/933528/jov-8-14-28.pdf},
}

@article{101001,
    author = {Cannon, Steve},
    title = {The Neurology of Eye Movements (Contemporary Neurology Series)},
    journal = {Archives of Ophthalmology},
    volume = {110},
    number = {3},
    pages = {326-326},
    year = {1992},
    month = {03},
    abstract = {During the last three decades the clinical recognition of eye movement disorders has advanced rapidly from descriptive phenomenology to accurate recording of eye movements and the presentation of specific pathophysiologic mechanisms for the abnormal movements observed in patients. This subject, which appears dauntingly complex to the uninitiated, was first distilled into a comprehensive yet clinically useful form by Leigh and Zee in the first edition of The Neurology of Eye Movements. Now, 8 years later, the second edition has incorporated the major advances made in the last decade, including those of 1990.The format of the second edition is identical to that of the first. The book is divided into two major sections: part 1 summarizes the results of basic research on eye movements, with an emphasis on the clinical relevance of this research, and part 2 builds on this framework to interpret the pathophysiologic mechanisms of the signs and},
    issn = {0003-9950},
    doi = {10.1001/archopht.1992.01080150024016},
    url = {https://doi.org/10.1001/archopht.1992.01080150024016},
    eprint = {https://jamanetwork.com/journals/jamaophthalmology/articlepdf/639553/archopht\_110\_3\_016.pdf},
}


@INPROCEEDINGS{8699248,
  author={Mohan, Pallavi and Goh, Wooi Boon and Fu, Chi-Wing and Yeung, Sai-Kit},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={DualGaze: Addressing the Midas Touch Problem in Gaze Mediated VR Interaction}, 
  year={2018},
  volume={},
  number={},
  pages={79-84},
  keywords={Gaze tracking;Headphones;Task analysis;Virtual reality;Trajectory;Back;Reliability engineering;virtual reality;interaction methods;gaze interaction;Midas touch;eye tracking;H.5.2 [Information Interfaces and Presentation]: User Interfaces—Interaction styles},
  doi={10.1109/ISMAR-Adjunct.2018.00039}}

@inproceedings{isomoto,
author = {Isomoto, Toshiya and Ando, Toshiyuki and Shizuki, Buntarou and Takahashi, Shin},
title = {Dwell time reduction technique using Fitts' law for gaze-based target acquisition},
year = {2018},

isbn = {9781450357067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204493.3204532},
doi = {10.1145/3204493.3204532},
abstract = {We present a dwell time reduction technique for gaze-based target acquisition. We adopt Fitts' Law to achieve the dwell time reduction. Our technique uses both the eye movement time for target acquisition estimated using Fitts' Law (Te) and the actual eye movement time (Ta) for target acquisition; a target is acquired when the difference between Te and Ta is small. First, we investigated the relation between the eye movement for target acquisition and Fitts' Law; the result indicated a correlation of 0.90 after error correction. Then we designed and implemented our technique. Finally, we conducted a user study to investigate the performance of our technique; an average dwell time of 86.7 ms was achieved, with a 10.0\% Midas-touch rate.},
booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \& Applications},
articleno = {26},
numpages = {7},
keywords = {ISO 9241, dwell-based interaction, eye movement, eye tracking, gaze interaction, midas-touch},
location = {Warsaw, Poland},
series = {ETRA '18}
}

@inproceedings{Chakraborty,
author = {Chakraborty, Tuhin and Sarcar, Sayan and Samanta, Debasis},
title = {Design and evaluation of a dwell-free eye typing technique},
year = {2014},
isbn = {9781450324748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559206.2581265},
doi = {10.1145/2559206.2581265},
abstract = {Dwelling, activated through gaze fixation for a prolonged time, is an essential task to be performed to select keys from on-screen keyboard present in the eye typing interface. Normally fixation on a key takes sufficient time which slows down eye typing rate. To get rid of it, researchers focused on minimizing or diminishing dwell time toward building a dwell-free interface. In this paper, we present an efficient dwell-free eye typing mechanism and compare it with a previous work with respect to text entry rate, learning rate and usability. The user experiment results reveal that newly proposed method performed slightly better than the other.},
booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},
pages = {1573–1578},
numpages = {6},
keywords = {text entry, eye typing, dwell-free eye typing},
location = {Toronto, Ontario, Canada},
series = {CHI EA '14}
}

@article{Tang02062025,
author = {Xiaoteng Tang and Xiaojiao Chen and Hanxi Leng and Zhengyu Wang and Bolin Chen and Yonghao Chen and Wenru Qi and Xiaosong Wang},
title = {Comparison and Optimization of Target-Assisted Gaze Input Technique for Enhanced Selection in Virtual Eye-Controlled Systems},
journal = {International Journal of Human–Computer Interaction},
volume = {0},
number = {0},
pages = {1--19},
year = {2025},
publisher = {Taylor \& Francis},
doi = {10.1080/10447318.2025.2505780},
URL = {https://doi.org/10.1080/10447318.2025.2505780},
eprint = {https://doi.org/10.1080/10447318.2025.2505780}
}

@article{Zhang02072020,
author = {Shaoyao Zhang and Yu Tian and Chunhui Wang and Kunlin Wei},
title = {Target selection by gaze pointing and manual confirmation: performance improved by locking the gaze cursor},
journal = {Ergonomics},
volume = {63},
number = {7},
pages = {884--895},
year = {2020},
publisher = {Taylor \& Francis},
doi = {10.1080/00140139.2020.1762934},
note ={PMID: 32348191},
URL = {https://doi.org/10.1080/00140139.2020.1762934},
eprint = {https://doi.org/10.1080/00140139.2020.1762934}}


@inproceedings{Vertegaal,
author = {Vertegaal, Roel},
title = {A Fitts Law comparison of eye tracking and manual input in the selection of visual targets},
year = {2008},
isbn = {9781605581989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1452392.1452443},
doi = {10.1145/1452392.1452443},
abstract = {We present a Fitts' Law evaluation of a number of eye tracking and manual input devices in the selection of large visual targets. We compared performance of two eye tracking techniques, manual click and dwell time click, with that of mouse and stylus. Results show eye tracking with manual click outperformed the mouse by 16\%, with dwell time click 46\% faster. However, eye tracking conditions suffered a high error rate of 11.7\% for manual click and 43\% for dwell time click conditions. After Welford correction eye tracking still appears to outperform manual input, with IPs of 13.8 bits/s for dwell time click, and 10.9 bits/s for manual click. Eye tracking with manual click provides the best tradeoff between speed and accuracy, and was preferred by 50\% of participants. Mouse and stylus had IPs of 4.7 and 4.2 respectively. However, their low error rate of 5\% makes these techniques more suitable for refined target selection.},
booktitle = {Proceedings of the 10th International Conference on Multimodal Interfaces},
pages = {241–248},
numpages = {8},
keywords = {input devices, focus selection, eye tracking, attentive user interfaces., Fitts Law},
location = {Chania, Crete, Greece},
series = {ICMI '08}
}

@inproceedings{Stellmach,
author = {Stellmach, Sophie and Dachselt, Raimund},
title = {Look \& touch: gaze-supported target acquisition},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2208709},
doi = {10.1145/2207676.2208709},
abstract = {While eye tracking has a high potential for fast selection tasks, it is often regarded as error-prone and unnatural, especially for gaze-only interaction. To improve on that, we propose gaze-supported interaction as a more natural and effective way combining a user's gaze with touch input from a handheld device. In particular, we contribute a set of novel and practical gaze-supported selection techniques for distant displays. Designed according to the principle gaze suggests, touch confirms they include an enhanced gaze-directed cursor, local zoom lenses and more elaborated techniques utilizing manual fine positioning of the cursor via touch. In a comprehensive user study with 24 participants, we investigated the potential of these techniques for different target sizes and distances. All novel techniques outperformed a simple gaze-directed cursor and showed individual advantages. In particular those techniques using touch for fine cursor adjustments (MAGIC touch) and for cycling through a list of possible close-to-gaze targets (MAGIC tab) demonstrated a high overall performance and usability.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2981–2990},
numpages = {10},
keywords = {target acquisition, selection, mobile touch interaction, gaze-supported interaction, gaze input},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@INPROCEEDINGS{Gherman,
  author={Gherman, Ovidiu and Schipor, Ovidiu and Gheran, Bogdan-Florin},
  booktitle={2018 International Conference on Development and Application Systems (DAS)}, 
  title={VErGE: A system for collecting voice, eye gaze, gesture, and EEG data for experimental studies}, 
  year={2018},
  volume={},
  number={},
  pages={150-155},
  keywords={Electroencephalography;Software;XML;Synchronization;Headphones;Data acquisition;Data collection;touch gestures;voice;EEG;eye gaze;eye tracker;motor impairments;experiments;experimental data},
  doi={10.1109/DAAS.2018.8396088}}

@INPROCEEDINGS{Saxena,
  author={Saxena, Shilpi and Ranjan, Mritunjay Kr. and Sattar, Arif Md.},
  booktitle={2024 1st International Conference on Cognitive, Green and Ubiquitous Computing (IC-CGU)}, 
  title={Brain-Computer Interfaces: A Key to Neural Communication's Limitless Possibilities}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  keywords={Human computer interaction;Electrodes;Ethics;Signal processing algorithms;Medical services;Ubiquitous computing;Brain-computer interfaces;Brain Computer Interface;Neural Signals;Human Machine Interaction;Real World Application;Virtual reality;Augmented reality},
  doi={10.1109/IC-CGU58078.2024.10530664}}

@inproceedings{Putze,
author = {Putze, Felix and Hild, Jutta and K\"{a}rgel, Rainer and Herff, Christian and Redmann, Alexander and Beyerer, J\"{u}rgen and Schultz, Tanja},
title = {Locating user attention using eye tracking and EEG for spatio-temporal event selection},
year = {2013},
isbn = {9781450319652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2449396.2449415},
doi = {10.1145/2449396.2449415},
abstract = {In expert video analysis, the selection of certain events in a continuous video stream is a frequently occurring operation, e.g., in surveillance applications. Due to the dynamic and rich visual input, the constantly high attention and the required hand-eye coordination for mouse interaction, this is a very demanding and exhausting task. Hence, relevant events might be missed. We propose to use eye tracking and electroencephalography (EEG) as additional input modalities for event selection. From eye tracking, we derive the spatial location of a perceived event and from patterns in the EEG signal we derive its temporal location within the video stream. This reduces the amount of the required active user input in the selection process, and thus has the potential to reduce the user's workload. In this paper, we describe the employed methods for the localization processes and introduce the developed scenario in which we investigate the feasibility of this approach. Finally, we present and discuss results on the accuracy and the speed of the method and investigate how the modalities interact.},
booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
pages = {129–136},
numpages = {8},
keywords = {eeg, event detection, expert video analysis, eye tracking},
location = {Santa Monica, California, USA},
series = {IUI '13}
}

@inproceedings{Putze2,
author = {Putze, Felix and Popp, Johannes and Hild, Jutta and Beyerer, J\"{u}rgen and Schultz, Tanja},
title = {Intervention-free selection using EEG and eye tracking},
year = {2016},
isbn = {9781450345569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993148.2993199},
doi = {10.1145/2993148.2993199},
abstract = {In this paper, we show how recordings of gaze movements (via eye tracking) and brain activity (via electroencephalography) can be combined to provide an interface for implicit selection in a graphical user interface. This implicit selection works completely without manual intervention by the user. In our approach, we formulate implicit selection as a classification problem, describe the employed features and classification setup and introduce our experimental setup for collecting evaluation data. With a fully online-capable setup, we can achieve an F_0.2-score of up to 0.74 for temporal localization and a spatial localization accuracy of more than 0.95.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimodal Interaction},
pages = {153–160},
numpages = {8},
keywords = {EEG, eye tracking, implicit selection},
location = {Tokyo, Japan},
series = {ICMI '16}
}

@inproceedings{Hild,
author = {Hild, Jutta and Putze, Felix and Kaufman, David and K\"{u}hnle, Christian and Schultz, Tanja and Beyerer, J\"{u}rgen},
title = {Spatio-Temporal Event Selection in Basic Surveillance Tasks using Eye Tracking and EEG},
year = {2014},
isbn = {9781450301251},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666642.2666645},
doi = {10.1145/2666642.2666645},
abstract = {In safety- and security-critical applications like video surveillance it is crucial that human operators detect task-relevant events in the continuous video streams and select them for report or dissemination to other authorities. Usually, the selection operation is performed using a manual input device like a mouse or a joystick. Due to the visually rich and dynamic input, the required high attention, the long working time, and the challenging manual selection of moving objects, it occurs that relevant events are missed. To alleviate this problem we propose adding another event selection process, using eye-brain input. Our approach is based on eye tracking and EEG, providing spatio-temporal event selection without any manual intervention. We report ongoing research, building on prior work where we showed the general feasibility of the approach. In this contribution, we extend our work testing the feasibility of the approach using more advanced and less artificial experimental paradigms simulating frequently occurring, basic types of real surveillance tasks. The paradigms are much closer to a real surveillance task in terms of the used visual stimuli, the more subtle cues for event indication, and the required viewing behavior. As a methodology we perform an experiment (N=10) with non-experts. The results confirm the feasibility of the approach for event selection in the advanced tasks. We achieve spatio-temporal event selection accuracy scores of up to 77\% and 60\% for different stages of event indication.},
booktitle = {Proceedings of the 7th Workshop on Eye Gaze in Intelligent Human Machine Interaction: Eye-Gaze \& Multimodality},
pages = {3–8},
numpages = {6},
keywords = {video surveillance task, eye tracking, experiment, event selection, event localization, eeg},
location = {Istanbul, Turkey},
series = {GazeIn '14}
}

@ARTICLE{Shishkin,
  
AUTHOR={Shishkin, Sergei L.  and Nuzhdin, Yuri O.  and Svirin, Evgeny P.  and Trofimov, Alexander G.  and Fedorova, Anastasia A.  and Kozyrskiy, Bogdan L.  and Velichkovsky, Boris M. },
TITLE={EEG Negativity in Fixations Used for Gaze-Based Control: Toward Converting Intentions into Actions with an Eye-Brain-Computer Interface},
JOURNAL={Frontiers in Neuroscience},
VOLUME={Volume 10 - 2016},
YEAR={2016},
URL={https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2016.00528},
DOI={10.3389/fnins.2016.00528},
ISSN={1662-453X},
ABSTRACT={We usually look at an object when we are going to manipulate it. Thus, eye tracking can be used to communicate intended actions. An effective human-machine interface, however, should be able to differentiate intentional and spontaneous eye movements. We report an electroencephalogram (EEG) marker that differentiates gaze fixations used for control from spontaneous fixations involved in visual exploration. Eight healthy participants played a game with their eye movements only. Their gaze-synchronized EEG data (fixation-related potentials, FRPs) were collected during game’s control-on and control-off conditions. A slow negative wave with a maximum in the parietooccipital region was present in each participant’s averaged FRPs in the control-on conditions and was absent or had much lower amplitude in the control-off condition. This wave was similar but not identical to stimulus-preceding negativity, a slow negative wave that can be observed during feedback expectation. Classification of intentional vs. spontaneous fixations was based on amplitude features from 13 EEG channels using 300 ms length segments free from electrooculogram contamination (200..500 ms relative to the fixation onset). For the first fixations in the fixation triplets required to make moves in the game, classified against control-off data, a committee of greedy classifiers provided 0.90 ± 0.07 specificity and 0.38 ± 0.14 sensitivity. Similar (slightly lower) results were obtained for the shrinkage LDA classifier. The second and third fixations in the triplets were classified at lower rate. We expect that, with improved feature sets and classifiers, a hybrid dwell-based Eye-Brain-Computer Interface (EBCI) can be built using the FRP difference between the intended and spontaneous fixations. If this direction of BCI development will be successful, such a multimodal interface may improve the fluency of interaction and can possibly become the basis for a new input device for paralyzed and healthy users, the EBCI “Wish Mouse”.}}

@article{kalaganis2018erroraware,
  author    = {Kalaganis, F. P. and Chatzilari, E. and Nikolopoulos, S. and Kompatsiaris, I.},
  title     = {An error-aware gaze-based keyboard by means of a hybrid BCI system},
  journal   = {Scientific Reports},
  volume    = {8},
  number    = {13176},
  year      = {2018},
  publisher = {Nature Publishing Group},
  doi       = {10.1038/s41598-018-31425-2},
  url       = {https://doi.org/10.1038/s41598-018-31425-2}
}

@ARTICLE{Vortmann,
AUTHOR={Vortmann, Lisa-Marie  and Ceh, Simon  and Putze, Felix },
TITLE={Multimodal EEG and Eye Tracking Feature Fusion Approaches for Attention Classification in Hybrid BCIs},    
JOURNAL={Frontiers in Computer Science},   
VOLUME={Volume 4 - 2022},
YEAR={2022},
URL={https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2022.780580},
DOI={10.3389/fcomp.2022.780580},
ISSN={2624-9898},
ABSTRACT={Often, various modalities capture distinct aspects of particular mental states or activities. While
machine learning algorithms can reliably predict numerous aspects of human cognition and
behavior using a single modality, they can benefit from the combination of multiple modalities.
This is why hybrid BCIs are gaining popularity. However, it is not always straightforward to combine
features from a multimodal dataset. Along with the method for generating the features, one must
decide when the modalities should be combined during the classification process. We compare
unimodal EEG and eye tracking classification of internally and externally directed attention to
multimodal approaches for early, middle, and late fusion in this study. On a binary dataset with a
chance level of 0.5, late fusion of the data achieves the highest classification accuracy of 0.609 to
0.675 (95%-confidence interval). In general, the results indicate that for these modalities, middle
or late fusion approaches are better suited than early fusion approaches. Additional validation
of the observed trend will require the use of additional datasets, alternative feature generation
mechanisms, decision rules, and neural network designs. We conclude with a set of premises that
need to be considered when deciding on a multimodal attentional state classification approach.}}

@ARTICLE{evain,
AUTHOR={Évain, Andéol  and Argelaguet, Ferran  and Casiez, Géry  and Roussel, Nicolas  and Lécuyer, Anatole },         
TITLE={Design and Evaluation of Fusion Approach for Combining Brain and Gaze Inputs for Target Selection},        
JOURNAL={Frontiers in Neuroscience},
VOLUME={Volume 10 - 2016},
YEAR={2016},
URL={https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2016.00454},
DOI={10.3389/fnins.2016.00454},
ISSN={1662-453X},
ABSTRACT={Gaze-based interfaces and Brain-Computer Interfaces (BCIs) allow for hands-free human-computer interaction. In this paper, we investigate the combination of gaze and brain-computer interfaces. We propose a novel selection technique for 2D target acquisition based on input fusion. This new approach combines the probabilistic models for each input, in order to better estimate the intent of the user. We evaluated its performance against the existing gaze and brain-computer interaction techniques. Twelve participants took part in our study, in which they had to search and select 2D targets with each of the evaluated techniques. Our fusion-based hybrid interaction technique was found to be more reliable than the previous gaze and BCI hybrid interaction techniques for 10 participants over 12, while being 29% faster on average. However, similarly to what has been observed in hybrid gaze-and-speech interaction, gaze-only interaction technique still provides the best performance. Our results should encourage the use of input fusion, as opposed to sequential interaction, in order to design better hybrid interfaces.}}

@INPROCEEDINGS{hou,
  author={Hou, Baosheng James and Abdrabou, Yasmeen and Weidner, Florian and Gellersen, Hans},
  booktitle={2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Unveiling Variations: A Comparative Study of VR Headsets Regarding Eye Tracking Volume, Gaze Accuracy, and Precision}, 
  year={2024},
  volume={},
  number={},
  pages={650-655},
  keywords={Headphones;Three-dimensional displays;Head-mounted displays;Decision making;Gaze tracking;Virtual reality;Resists;Human-centered computing—Human-Computer-Interaction—Interaction paradigms—Mixed / augmented reality;Human-centered computing—Human-Computer-Interaction—Interaction paradigms—Virtual reality},
  doi={10.1109/VRW62533.2024.00127}}

@manual{emotiv2020epocx,
  title        = {Emotiv EPOC X Technical Specifications},
  author       = {{Emotiv Inc.}},
  year         = {2020},
  url          = {https://emotiv.gitbook.io/epoc-x-user-manual/introduction/technical-specifications},
  note         = {Internal sampling: 2048 Hz; downsampled to 128/256 Hz for BLE or other transmission}
}


@mastersthesis{Barbel2024,
  author       = {Wanyea Barbel},
  title        = {NeuroGaze in Virtual Reality: Assessing an EEG and Eye Tracking Interface against Traditional Virtual Reality Input Devices},
  school       = {University of Central Florida},
  year         = {2024},
  type         = {Master's Thesis},
  address      = {Orlando, FL},
  url          = {https://stars.library.ucf.edu/etd2023/184},
  note         = {Graduate Thesis and Dissertation 2023--2024, STARS Digital Repository}
}


________
